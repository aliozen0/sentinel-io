from fastapi import FastAPI, HTTPException, WebSocket, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles

from pydantic import BaseModel

from typing import List, Dict, Optional

import os
import logging
import asyncio
import uuid

from auth import get_current_user, create_local_token
from db.client import get_db
from fastapi import Depends, Security


# --- io-Guard v1.0 Core Imports (FAIL-FAST: Missing modules crash at startup) ---
from state_manager import state
from services.orchestrator import AgentOrchestrator

from logger import get_logger, LOG_FILE

# New Agents
from agents.auditor import Auditor, AuditReport
from agents.sniper import Sniper, GPUNode
from agents.architect import Architect, EnvironmentConfig
from agents.executor import Executor

# New Agents
from agents.auditor import Auditor, AuditReport
from agents.sniper import Sniper, GPUNode
from agents.architect import Architect, EnvironmentConfig
from agents.executor import Executor

# Jobs
from jobs.analysis_job import run_analysis_bg

# Demo Routes
from routes import demo


logger = logging.getLogger("io-guard-core") # Updated logger name for v1.0

# ==========================================
# ðŸ” HYBRID JOB MANAGER (Persistent + In-Memory)
# ==========================================

class JobManager:
    """
    Manages job state with hybrid storage:
    - Persistent DB: Job metadata (id, status, type, timestamps)
    - In-Memory: Sensitive credentials (keys, passwords) - TTL limited
    
    Credentials are stored in memory ONLY for the duration of the request,
    and cleared after job completion or timeout.
    """
    _credentials_cache: dict = {}  # {job_id: {credentials...}}
    
    @classmethod
    def create_job(cls, job_id: str, job_type: str, config: dict) -> bool:
        """Create job: store metadata in DB, credentials in memory."""
        db = get_db()
        
        # Store non-sensitive metadata in DB
        metadata = {
            "hostname": config.get("hostname"),
            "username": config.get("username"),
            "port": config.get("port"),
            "auth_type": config.get("auth_type"),
            "script_path": config.get("script_path"),
            "project_dir": config.get("project_dir"),
            "entry_point": config.get("entry_point"),
            "install_requirements": config.get("install_requirements"),
            "type": config.get("type")
        }
        db.create_job(job_id, job_type, config.get("status", "PENDING"), metadata)
        
        # Store credentials in memory (NEVER persisted)
        cls._credentials_cache[job_id] = {
            "private_key": config.get("private_key"),
            "password": config.get("password"),
            "passphrase": config.get("passphrase"),
            # Also cache full config for websocket handler
            **config
        }
        
        return True
    
    @classmethod
    def get_job_config(cls, job_id: str) -> Optional[dict]:
        """Get full job config (merged from DB + memory cache)."""
        if job_id in cls._credentials_cache:
            return cls._credentials_cache[job_id]
        return None
    
    @classmethod
    def update_status(cls, job_id: str, status: str):
        """Update job status in DB."""
        db = get_db()
        db.update_job_status(job_id, status)
        
        # Update in-memory cache too
        if job_id in cls._credentials_cache:
            cls._credentials_cache[job_id]["status"] = status
    
    @classmethod
    def cleanup_job(cls, job_id: str):
        """Remove credentials from memory after job completes."""
        cls._credentials_cache.pop(job_id, None)


app = FastAPI(title="io-Guard Core (v1.0)")

# Singleton Orchestrator
orchestrator = AgentOrchestrator()

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Register Demo Routes
app.include_router(demo.router, prefix="/v1/connections", tags=["Demo"])

# Ensure uploads directory exists
UPLOADS_DIR = os.path.join(os.path.dirname(__file__), "uploads")
os.makedirs(UPLOADS_DIR, exist_ok=True)

# Mount static files for serving uploads
app.mount("/uploads", StaticFiles(directory=UPLOADS_DIR), name="uploads")

# ==========================================
# ðŸš€ io-Guard v1.0 API ENDPOINTS (THE NEW CORE)
# ==========================================

# Available io.net models
IO_NET_MODELS = [
    {"id": "deepseek-ai/DeepSeek-V3.2", "name": "DeepSeek V3.2", "type": "chat", "recommended": True},
    {"id": "deepseek-ai/DeepSeek-R1-0528", "name": "DeepSeek R1 (Reasoning)", "type": "reasoning"},
    {"id": "meta-llama/Llama-3.3-70B-Instruct", "name": "Llama 3.3 70B", "type": "chat"},
    {"id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "name": "Llama 4 Maverick", "type": "chat"},
    {"id": "Qwen/Qwen3-235B-A22B-Thinking-2507", "name": "Qwen3 235B Thinking", "type": "reasoning"},
    {"id": "Qwen/Qwen2.5-VL-32B-Instruct", "name": "Qwen2.5 VL 32B (Vision)", "type": "vision"},
    {"id": "moonshotai/Kimi-K2-Thinking", "name": "Kimi K2 Thinking", "type": "reasoning"},
    {"id": "moonshotai/Kimi-K2-Instruct-0905", "name": "Kimi K2 Instruct", "type": "chat"},
    {"id": "mistralai/Mistral-Large-Instruct-2411", "name": "Mistral Large", "type": "chat"},
    {"id": "mistralai/Mistral-Nemo-Instruct-2407", "name": "Mistral Nemo", "type": "chat"},
    {"id": "mistralai/Devstral-Small-2505", "name": "Devstral Small (Code)", "type": "code"},
    {"id": "openai/gpt-oss-120b", "name": "GPT OSS 120B", "type": "chat"},
    {"id": "zai-org/GLM-4.7", "name": "GLM 4.7", "type": "chat"},
]

# --- Credit Cost Configuration ---
CREDIT_COSTS = {
    "analyze": 0.50,      # Code analysis
    "chat": 0.10,         # Chat message
    "deploy_start": 1.00, # Deployment initiation
    "deploy_minute": 0.05 # Per minute during execution
}

# --- Models ---
class AnalyzeRequest(BaseModel):
    code: str
    budget: float = 10.0
    model: Optional[str] = None  # Custom model override

class ChatRequest(BaseModel):
    messages: List[Dict[str, str]]
    model: Optional[str] = None

class DeploySimulateRequest(BaseModel):
    job_config: Dict



# --- System & Auth Endpoints ---

@app.get("/v1/system/status")
async def system_status():
    """Returns the current operating mode (CLOUD vs LOCAL)."""
    db = get_db()
    return {"mode": db.mode}

# Include Auth Router
from routes import auth_routes as auth_router
app.include_router(auth_router.router, prefix="/v1/auth", tags=["Auth"])

# --- Model Selection Endpoint ---

# --- Model Selection Endpoint ---


@app.get("/v1/models")
async def get_available_models():
    """Returns available io.net AI models for analysis."""
    default_model = os.getenv("IO_MODEL_NAME", "deepseek-ai/DeepSeek-V3.2")
    return {
        "default_model": default_model,
        "models": IO_NET_MODELS
    }

# --- Endpoints ---

@app.post("/v1/analyze")
async def analyze_code(request: AnalyzeRequest, current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Starts background code analysis job.
    Returns job_id for WebSocket tracking.
    """
    import uuid
    
    user_id = current_user.get("id")
    cost = CREDIT_COSTS["analyze"]
    user_credits = float(current_user.get("credits", 0.0))
    
    if user_credits < cost:
        raise HTTPException(status_code=402, detail=f"Insufficient credits. Required: {cost}, Available: {user_credits:.2f}")
    
    # Deduct credits upfront
    db = get_db()
    if not db.deduct_credits(user_id, cost, "Code Analysis"):
        raise HTTPException(status_code=402, detail="Credit deduction failed")

    # Create Job
    job_id = f"audit_{uuid.uuid4().hex[:8]}"
    default_model = os.getenv("IO_MODEL_NAME", "deepseek-ai/DeepSeek-V3.2")
    model_name = request.model if request.model else default_model

    metadata = {
        "user_id": user_id,
        "mode": "ANALYSIS",
        "code_snippet": request.code[:100] + "...", 
        "code": request.code, # Store full code for session restoration
        "model": model_name,
        "input_budget": request.budget,
        "pipeline_trace": [],
        "final_result": None
    }

    if not db.create_job(job_id, "ANALYSIS", "PENDING", metadata):
         # Refund if job creation fails
         db.add_credits(user_id, cost, "Refund: Job Init Failed")
         raise HTTPException(status_code=500, detail="Failed to initialize analysis job")

    # Start Background Task
    asyncio.create_task(run_analysis_bg(
        job_id=job_id,
        code=request.code,
        user_id=user_id,
        budget=request.budget,
        model_name=model_name
    ))
    
    
    return {"job_id": job_id, "status": "PENDING"}


@app.get("/v1/analyze/history")
async def get_analyze_history(current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Returns the user's analysis job history.
    Useful for restoring session state.
    """
    user_id = current_user.get("id")
    db = get_db()
    
    jobs = db.get_user_jobs(user_id)
    
    # Filter for analysis jobs only
    analysis_jobs = [j for j in jobs if j.get("mode") == "ANALYSIS"]
    
    # Parse metadata strings if local mode
    for job in analysis_jobs:
        if isinstance(job.get("metadata"), str):
            import json
            try:
                job["metadata"] = json.loads(job["metadata"])
            except:
                job["metadata"] = {}
                
    return {"jobs": analysis_jobs}


@app.websocket("/ws/analyze/{job_id}")
async def websocket_analyze(websocket: WebSocket, job_id: str):
    await websocket.accept()
    db = get_db()
    try:
        last_status = None
        last_trace_len = 0
        
        while True:
            job = db.get_job(job_id)
            if not job:
                await websocket.send_json({"error": "Job not found"})
                break
                
            status = job.get("status")
            # Parse metadata if it's a string (Local Mode) or use as is (Cloud Mode)
            meta = job.get("metadata", {})
            if isinstance(meta, str):
                import json
                try: meta = json.loads(meta)
                except: meta = {}
            
            trace = meta.get("pipeline_trace", [])
            final_result = meta.get("final_result")
            
            # Send full update if something changed
            if status != last_status or len(trace) != last_trace_len or final_result:
                payload = {
                    "status": status,
                    "pipeline_trace": trace,
                    "result": final_result
                }
                await websocket.send_json(payload)
                
                last_status = status
                last_trace_len = len(trace)
            
            if status in ["COMPLETED", "FAILED"]:
                break
                
            await asyncio.sleep(1) # Poll interval
            
    except Exception as e:
        logger.error(f"WS Analyze Error: {e}")
    finally:
        try: await websocket.close()
        except: pass

# Note: get_db is already imported at the top of the file


@app.post("/v1/chat")
async def chat_agent(request: ChatRequest, current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Project-Aware AI Chatbot.
    Uses ChatAgent to provide context-aware responses (Analysis, Deployment, etc.)
    """
    from agents.chat import ChatAgent
    
    if not request.messages:
        raise HTTPException(status_code=400, detail="No messages provided")
    
    # Credit check and deduction for chat
    user_id = current_user.get("id")
    cost = CREDIT_COSTS["chat"]
    user_credits = float(current_user.get("credits", 0.0))
    
    if user_credits < cost:
        raise HTTPException(
            status_code=402, 
            detail=f"Yetersiz kredi. Chat maliyeti: {cost}, Mevcut: {user_credits:.2f}"
        )
    
    db = get_db()
    db.deduct_credits(user_id, cost, "Chat Message")
    
    # 1. Persist User Message (Real DB)
    if db:
        try:
            db.log_chat(user_id, "user", request.messages[-1]["content"])
        except: pass

    # 2. Get Response from ChatAgent (Context Aware)
    # Get model from request or fallback
    default_model = os.getenv("IO_MODEL_NAME", "deepseek-ai/DeepSeek-V3.2")
    model_name = request.model if request.model else default_model
    
    response_content = await ChatAgent.chat(request.messages, model=model_name)
    
    # 3. Persist AI Response
    if db:
        try:
            db.log_chat(user_id, "assistant", response_content)
        except: pass

    return {"role": "assistant", "content": response_content}

@app.get("/v1/market/status")
async def market_status():
    """
    [v1.0] Returns cached market data or status.
    """
    data = Sniper._get_market_data()
    return {"source": "live/cache", "node_count": len(data), "sample": data[:10]}

# --- Terminal WebSocket ---
from agents.terminal import terminal_manager
from fastapi import WebSocket, WebSocketDisconnect
import asyncio
import json

@app.websocket("/ws/terminal")
async def terminal_websocket(websocket: WebSocket):
    await websocket.accept()
    session_id = id(websocket) 
    
    try:
        # Wait for init message
        data = await websocket.receive_text()
        msg = json.loads(data)
        
        if msg.get("type") == "connect":
            config = msg.get("config")
            try:
                terminal_manager.create_session(session_id, config)
                await websocket.send_json({"type": "status", "data": "connected"})
            except Exception as e:
                await websocket.send_json({"type": "error", "data": str(e)})
                return
        
        # Main Loop
        session = terminal_manager.get_session(session_id)
        if not session:
            return

        # Background task to read from SSH
        async def read_ssh():
            while True:
                output = session.recv()
                if output:
                    await websocket.send_json({"type": "output", "data": output})
                else:
                    await asyncio.sleep(0.1)

        reader_task = asyncio.create_task(read_ssh())
        
        while True:
            data = await websocket.receive_text()
            msg = json.loads(data)
            
            if msg.get("type") == "input":
                session.send(msg.get("data"))
            elif msg.get("type") == "resize":
                # Handle resize if needed
                pass

    except WebSocketDisconnect:
        terminal_manager.close_session(session_id)
    except Exception as e:
        logger.error(f"WS Error: {e}")
        terminal_manager.close_session(session_id)

@app.post("/v1/deploy/simulate")
async def deploy_simulate(request: DeploySimulateRequest):
    """
    [v1.0] Starts a simulation job.
    """
    import uuid
    job_id = f"sim_{str(uuid.uuid4())[:8]}"
    return {"job_id": job_id, "status": "simulation_started"}

@app.post("/v1/deploy/live")
async def deploy_live(payload: dict):
    """
    [v1.0] Mock for Live Deployment.
    """
    return {"job_id": "live_999", "status": "pending_ssh_connection"}

class SSHConnectionRequest(BaseModel):
    hostname: str
    username: str
    port: int = 22
    auth_type: str = "key"  # "key" or "password"
    private_key: Optional[str] = None
    password: Optional[str] = None
    passphrase: Optional[str] = None  # For encrypted keys

@app.post("/v1/connection/test")
async def test_ssh_connection(request: SSHConnectionRequest):
    """
    [v1.0] Tests SSH connection to a remote node.
    Supports: Private Key, Password, and Passphrase-protected keys.
    """
    try:
        from services.ssh_manager import SSHManager
        success, message = await SSHManager.test_connection(
            hostname=request.hostname, 
            username=request.username, 
            port=request.port,
            auth_type=request.auth_type,
            private_key=request.private_key,
            password=request.password,
            passphrase=request.passphrase
        )
        return {"success": success, "message": message}
    except ImportError:
        return {"success": False, "message": "SSH Module (Paramiko) missing. Please rebuild backend."}
    except Exception as e:
        logger.error(f"SSH Test Error: {e}")
        return {"success": False, "message": f"Server Error: {str(e)}"}

# --- File Upload ---

ALLOWED_EXTENSIONS = {'.py', '.txt', '.yaml', '.yml', '.json', '.zip'}

def get_file_type(filename: str) -> str:
    """Determine file type from extension."""
    if filename.endswith('.py'):
        return 'script'
    elif filename.endswith('.txt'):
        return 'requirements'
    elif filename.endswith(('.yaml', '.yml')):
        return 'config'
    elif filename.endswith('.json'):
        return 'config'
    elif filename.endswith('.zip'):
        return 'archive'
    return 'other'

def find_entry_point(directory: str) -> Optional[str]:
    """Find the main Python script in a directory."""
    # Priority: main.py > train.py > run.py > first .py file
    priority_names = ['main.py', 'train.py', 'run.py', 'app.py', 'script.py']
    
    for name in priority_names:
        path = os.path.join(directory, name)
        if os.path.exists(path):
            return name
    
    # Fallback to first .py file
    for f in os.listdir(directory):
        if f.endswith('.py'):
            return f
    
    return None

@app.post("/v1/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    [v1.0] Uploads a single file for remote execution.
    Accepts: .py, .txt, .yaml, .yml, .json, .zip
    """
    # Security: Check allowed extensions
    ext = os.path.splitext(file.filename)[1].lower()
    if ext not in ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=400, 
            detail=f"Only {', '.join(ALLOWED_EXTENSIONS)} files are allowed"
        )
    
    # Generate unique filename
    file_id = str(uuid.uuid4())[:8]
    safe_filename = f"{file_id}_{file.filename.replace(' ', '_')}"
    file_path = os.path.join(UPLOADS_DIR, safe_filename)
    
    try:
        # Save file
        contents = await file.read()
        with open(file_path, "wb") as f:
            f.write(contents)
        
        file_url = f"http://localhost:8000/uploads/{safe_filename}"
        
        logger.info(f"File uploaded: {safe_filename} ({len(contents)} bytes)")
        
        return {
            "success": True,
            "filename": safe_filename,
            "original_name": file.filename,
            "size": len(contents),
            "url": file_url,
            "local_path": file_path,
            "type": get_file_type(file.filename)
        }
    except Exception as e:
        logger.error(f"Upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.post("/v1/upload/project")
async def upload_project(files: List[UploadFile] = File(...)):
    """
    [v1.0] Uploads multiple files or a ZIP archive as a project.
    Creates a project directory and extracts files.
    Returns project metadata including entry point.
    """
    import zipfile
    import io as io_module
    
    project_id = str(uuid.uuid4())[:8]
    project_dir = os.path.join(UPLOADS_DIR, f"project_{project_id}")
    os.makedirs(project_dir, exist_ok=True)
    
    uploaded_files = []
    
    try:
        for file in files:
            ext = os.path.splitext(file.filename)[1].lower()
            
            if ext == '.zip':
                # Extract ZIP archive
                contents = await file.read()
                try:
                    with zipfile.ZipFile(io_module.BytesIO(contents), 'r') as zip_ref:
                        # Security: Check for path traversal
                        for member in zip_ref.namelist():
                            if member.startswith('..') or member.startswith('/'):
                                raise HTTPException(status_code=400, detail="Invalid ZIP: path traversal detected")
                        
                        zip_ref.extractall(project_dir)
                        
                        # List extracted files
                        for root, dirs, files_list in os.walk(project_dir):
                            for f in files_list:
                                rel_path = os.path.relpath(os.path.join(root, f), project_dir)
                                full_path = os.path.join(root, f)
                                uploaded_files.append({
                                    "filename": rel_path,
                                    "size": os.path.getsize(full_path),
                                    "type": get_file_type(f),
                                    "source": "zip"
                                })
                        
                        logger.info(f"ZIP extracted: {file.filename} -> {len(uploaded_files)} files")
                except zipfile.BadZipFile:
                    raise HTTPException(status_code=400, detail="Invalid ZIP file")
            
            elif ext in ALLOWED_EXTENSIONS:
                # Regular file
                safe_filename = file.filename.replace(' ', '_')
                file_path = os.path.join(project_dir, safe_filename)
                
                contents = await file.read()
                with open(file_path, "wb") as f:
                    f.write(contents)
                
                uploaded_files.append({
                    "filename": safe_filename,
                    "size": len(contents),
                    "type": get_file_type(file.filename),
                    "source": "upload"
                })
                
                logger.info(f"File added to project: {safe_filename}")
            else:
                logger.warning(f"Skipped unsupported file: {file.filename}")
        
        # Find entry point
        entry_point = find_entry_point(project_dir)
        
        # Count files by type
        script_count = sum(1 for f in uploaded_files if f['type'] == 'script')
        
        return {
            "success": True,
            "project_id": project_id,
            "project_dir": project_dir,
            "files": uploaded_files,
            "file_count": len(uploaded_files),
            "script_count": script_count,
            "entry_point": entry_point,
            "has_requirements": any(f['type'] == 'requirements' for f in uploaded_files)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Project upload error: {e}")
        # Cleanup on error
        import shutil
        if os.path.exists(project_dir):
            shutil.rmtree(project_dir, ignore_errors=True)
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.get("/v1/uploads")
async def list_uploads():
    """
    [v1.0] Lists all uploaded files and projects.
    """
    try:
        files = []
        projects = []
        
        for item in os.listdir(UPLOADS_DIR):
            item_path = os.path.join(UPLOADS_DIR, item)
            
            if os.path.isdir(item_path) and item.startswith('project_'):
                # It's a project directory
                project_files = []
                for root, dirs, files_list in os.walk(item_path):
                    for f in files_list:
                        rel_path = os.path.relpath(os.path.join(root, f), item_path)
                        project_files.append(rel_path)
                
                projects.append({
                    "project_id": item.replace('project_', ''),
                    "path": item_path,
                    "file_count": len(project_files),
                    "entry_point": find_entry_point(item_path)
                })
            else:
                # Regular file
                ext = os.path.splitext(item)[1].lower()
                if ext in ALLOWED_EXTENSIONS:
                    files.append({
                        "filename": item,
                        "size": os.path.getsize(item_path),
                        "url": f"http://localhost:8000/uploads/{item}",
                        "type": get_file_type(item)
                    })
        
        return {"files": files, "projects": projects}
    except Exception as e:
        return {"files": [], "projects": [], "error": str(e)}


# --- Remote Execution ---

class ExecuteRequest(BaseModel):
    hostname: str
    username: str
    port: int = 22
    auth_type: str = "key"  # "key" or "password"
    private_key: Optional[str] = None
    password: Optional[str] = None
    passphrase: Optional[str] = None
    script_path: str  # Local path to the uploaded script


@app.post("/v1/deploy/execute")
async def deploy_execute(request: ExecuteRequest, current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Uploads script to remote server and executes it.
    Supports: Private Key, Password, and Passphrase-protected keys.
    Credentials are kept in-memory only, never persisted to disk.
    """
    if not os.path.exists(request.script_path):
        raise HTTPException(status_code=404, detail="Script file not found")
    
    # Credit check and deduction for deployment
    user_id = current_user.get("id")
    cost = CREDIT_COSTS["deploy_start"]
    user_credits = float(current_user.get("credits", 0.0))
    
    if user_credits < cost:
        raise HTTPException(
            status_code=402, 
            detail=f"Yetersiz kredi. Deploy maliyeti: {cost}, Mevcut: {user_credits:.2f}"
        )
    
    db = get_db()
    db.deduct_credits(user_id, cost, "Deployment Start")
    
    job_id = f"exec_{str(uuid.uuid4())[:8]}"
    
    # Use JobManager for hybrid storage (metadata in DB, credentials in memory)
    JobManager.create_job(job_id, "LIVE", {
        "hostname": request.hostname,
        "username": request.username,
        "port": request.port,
        "auth_type": request.auth_type,
        "private_key": request.private_key,
        "password": request.password,
        "passphrase": request.passphrase,
        "script_path": request.script_path,
        "status": "PENDING",
        "user_id": user_id  # Store user_id for per-minute billing
    })
    
    logger.info(f"Execution job created: {job_id} (charged {cost} credits)")
    
    return {
        "job_id": job_id,
        "status": "pending",
        "message": "Connect to WebSocket /ws/execute/{job_id} for logs",
        "credits_charged": cost
    }


@app.websocket("/ws/execute/{job_id}")
async def websocket_execute(websocket: WebSocket, job_id: str):
    """
    [v1.0] Real-time execution logs via WebSocket.
    Uploads file to remote server and executes it.
    """
    await websocket.accept()
    
    try:
        config = JobManager.get_job_config(job_id)
        if not config:
            await websocket.send_text("âŒ Job not found or credentials expired")
            await websocket.close()
            return
        
        JobManager.update_status(job_id, "RUNNING")
        await websocket.send_text(f"ðŸš€ Starting job: {job_id}")
        
        from services.ssh_manager import SSHManager
        
        async for line in SSHManager.upload_and_execute(
            hostname=config["hostname"],
            username=config["username"],
            local_path=config["script_path"],
            port=config["port"],
            auth_type=config["auth_type"],
            private_key=config.get("private_key"),
            password=config.get("password"),
            passphrase=config.get("passphrase")
        ):
            await websocket.send_text(line)
        
        
        # Success case - we need to mark as COMPLETED if loop finished without error
        JobManager.update_status(job_id, "COMPLETED")
        await websocket.send_text(f"âœ… Job {job_id} completed") 
        
    except Exception as e:
        logger.error(f"Execution error: {e}")
        JobManager.update_status(job_id, "FAILED")
        await websocket.send_text(f"âŒ Error: {str(e)}")
    finally:
        # Clean up credentials from memory
        JobManager.cleanup_job(job_id)
        try:
            await websocket.close()
        except:
            pass


# --- Project Execution ---

class ExecuteProjectRequest(BaseModel):
    hostname: str
    username: str
    port: int = 22
    auth_type: str = "key"
    private_key: Optional[str] = None
    password: Optional[str] = None
    passphrase: Optional[str] = None
    project_dir: str  # Local path to the project directory
    entry_point: str  # Main script to execute (e.g., "main.py")
    install_requirements: bool = True


@app.post("/v1/deploy/project")
async def deploy_project(request: ExecuteProjectRequest):
    """
    [v1.0] Uploads a project to remote server and executes it.
    Supports: Multiple files, requirements.txt installation, entry point execution.
    Credentials are kept in-memory only, never persisted.
    """
    if not os.path.exists(request.project_dir):
        raise HTTPException(status_code=404, detail="Project directory not found")
    
    entry_path = os.path.join(request.project_dir, request.entry_point)
    if not os.path.exists(entry_path):
        raise HTTPException(status_code=404, detail=f"Entry point '{request.entry_point}' not found in project")
    
    job_id = f"proj_{str(uuid.uuid4())[:8]}"
    
    # Use JobManager for hybrid storage
    JobManager.create_job(job_id, "project", {
        "type": "project",
        "hostname": request.hostname,
        "username": request.username,
        "port": request.port,
        "auth_type": request.auth_type,
        "private_key": request.private_key,
        "password": request.password,
        "passphrase": request.passphrase,
        "project_dir": request.project_dir,
        "entry_point": request.entry_point,
        "install_requirements": request.install_requirements,
        "status": "pending"
    })
    
    logger.info(f"Project execution job created: {job_id}")
    
    return {
        "job_id": job_id,
        "status": "pending",
        "message": "Connect to WebSocket /ws/execute-project/{job_id} for logs"
    }


@app.websocket("/ws/execute-project/{job_id}")
async def websocket_execute_project(websocket: WebSocket, job_id: str):
    """
    [v1.0] Real-time project execution logs via WebSocket.
    Uploads entire project, installs requirements, and executes entry point.
    """
    await websocket.accept()
    
    try:
        config = JobManager.get_job_config(job_id)
        if not config:
            await websocket.send_text("âŒ Job not found or credentials expired")
            await websocket.close()
            return
        
        if config.get("type") != "project":
            await websocket.send_text("âŒ Invalid job type for this endpoint")
            await websocket.close()
            return
        
        JobManager.update_status(job_id, "running")
        await websocket.send_text(f"ðŸš€ Starting project job: {job_id}")
        
        from services.ssh_manager import SSHManager
        
        async for line in SSHManager.upload_project_and_execute(
            hostname=config["hostname"],
            username=config["username"],
            project_dir=config["project_dir"],
            entry_point=config["entry_point"],
            port=config["port"],
            auth_type=config["auth_type"],
            private_key=config.get("private_key"),
            password=config.get("password"),
            passphrase=config.get("passphrase"),
            install_requirements=config.get("install_requirements", True)
        ):
            await websocket.send_text(line)
        
        JobManager.update_status(job_id, "COMPLETED")
        await websocket.send_text(f"âœ… Project job {job_id} completed")
        
    except Exception as e:
        logger.error(f"Project execution error: {e}")
        JobManager.update_status(job_id, "FAILED")
        await websocket.send_text(f"âŒ Error: {str(e)}")
    finally:
        # Clean up credentials from memory
        JobManager.cleanup_job(job_id)
        try:
            await websocket.close()
        except:
            pass


@app.websocket("/ws/logs/{job_id}")
async def websocket_logs(websocket: WebSocket, job_id: str):
    """
    [v1.0] Real-time logs for jobs (Simulation/Live).
    """
    await websocket.accept()
    try:
        async for line in Executor.run_simulation({"job_id": job_id}):
             await websocket.send_text(line)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        try:
            await websocket.close()
        except:
            pass

@app.get("/v1/dashboard/stats")
async def dashboard_stats(current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Aggregated real-time stats for the dashboard.
    Returns: Agents, Market Status, Financials, System Health.
    """
    # 1. Market Data (Real from io.net)
    market_data = Sniper._get_market_data()
    node_count = sum(n.get("total_nodes", 0) for n in market_data) if market_data else 0
    if node_count == 0: node_count = len(market_data) # Fallback if total_nodes missing

    # 2. Financials (Real from DB)
    real_balance = float(current_user.get("credits", 0.0))
    # We remove the mock burn rate calculation as requested
    est_hourly_burn = 0.0 
    # Estimate burn: sum of active job budgets or placeholder
    est_hourly_burn = 0.45 * 4 # Example: 4 active nodes x $0.45 avg

    # 3. Agent Status
    # In a real system, we'd check if specific asyncio tasks are running.
    # Here we assume core agents are "Active" if the app is up.
    active_agents = 4 # Auditor, Sniper, Architect, Executor

    # 4. Alerts
    # Count recent error logs
    recent_logs = state.get_agent_logs(limit=20)
    error_count = sum(1 for log in recent_logs if "error" in str(log).lower() or "warning" in str(log).lower())

    return {
        "active_agents": active_agents,
        "market_nodes": node_count,
        "market_sample": market_data[:20], # Top 20 for snapshot
        "system_health": 100 if error_count == 0 else 90, # Simple logic
        "alerts_count": error_count,
        "financials": {
            "balance": real_balance,
            "currency": "USDC",
            "hourly_burn": est_hourly_burn,
            "rewards_24h": 0.0 # Placeholder/Future implementation
        }
    }


# ==========================================
# ðŸ End of v1.0 Core
# ==========================================

