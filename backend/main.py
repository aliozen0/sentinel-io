from fastapi import FastAPI, HTTPException, WebSocket, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles

from pydantic import BaseModel

from typing import List, Dict, Optional

import os
import logging
import asyncio
import uuid

from auth import get_current_user, create_local_token
from db.client import get_db
from fastapi import Depends, Security


# --- io-Guard v1.0 Core Imports ---
try:

    from state_manager import state
    from services.orchestrator import AgentOrchestrator

    from logger import get_logger, LOG_FILE
    
    # New Agents
    from agents.auditor import Auditor, AuditReport
    from agents.sniper import Sniper, GPUNode
    from agents.architect import Architect, EnvironmentConfig
    from agents.executor import Executor
    
    # Demo Routes
    from routes import demo
except ImportError:
    # Fallback for local dev if paths are tricky
    pass

logger = logging.getLogger("io-guard-core") # Updated logger name for v1.0

app = FastAPI(title="io-Guard Core (v1.0 + Legacy v2.0)")

# Singleton Orchestrator
orchestrator = AgentOrchestrator()

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Register Demo Routes
app.include_router(demo.router, prefix="/v1/connections", tags=["Demo"])

# Ensure uploads directory exists
UPLOADS_DIR = os.path.join(os.path.dirname(__file__), "uploads")
os.makedirs(UPLOADS_DIR, exist_ok=True)

# Mount static files for serving uploads
app.mount("/uploads", StaticFiles(directory=UPLOADS_DIR), name="uploads")

# ==========================================
# üöÄ io-Guard v1.0 API ENDPOINTS (THE NEW CORE)
# ==========================================

# Available io.net models
IO_NET_MODELS = [
    {"id": "deepseek-ai/DeepSeek-V3.2", "name": "DeepSeek V3.2", "type": "chat", "recommended": True},
    {"id": "deepseek-ai/DeepSeek-R1-0528", "name": "DeepSeek R1 (Reasoning)", "type": "reasoning"},
    {"id": "meta-llama/Llama-3.3-70B-Instruct", "name": "Llama 3.3 70B", "type": "chat"},
    {"id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "name": "Llama 4 Maverick", "type": "chat"},
    {"id": "Qwen/Qwen3-235B-A22B-Thinking-2507", "name": "Qwen3 235B Thinking", "type": "reasoning"},
    {"id": "Qwen/Qwen2.5-VL-32B-Instruct", "name": "Qwen2.5 VL 32B (Vision)", "type": "vision"},
    {"id": "moonshotai/Kimi-K2-Thinking", "name": "Kimi K2 Thinking", "type": "reasoning"},
    {"id": "moonshotai/Kimi-K2-Instruct-0905", "name": "Kimi K2 Instruct", "type": "chat"},
    {"id": "mistralai/Mistral-Large-Instruct-2411", "name": "Mistral Large", "type": "chat"},
    {"id": "mistralai/Mistral-Nemo-Instruct-2407", "name": "Mistral Nemo", "type": "chat"},
    {"id": "mistralai/Devstral-Small-2505", "name": "Devstral Small (Code)", "type": "code"},
    {"id": "openai/gpt-oss-120b", "name": "GPT OSS 120B", "type": "chat"},
    {"id": "zai-org/GLM-4.7", "name": "GLM 4.7", "type": "chat"},
]

# --- Models ---
class AnalyzeRequest(BaseModel):
    code: str
    budget: float = 10.0
    model: Optional[str] = None  # Custom model override

class ChatRequest(BaseModel):
    messages: List[Dict[str, str]]

class DeploySimulateRequest(BaseModel):
    job_config: Dict



# --- System & Auth Endpoints ---

@app.get("/v1/system/status")
async def system_status():
    """Returns the current operating mode (CLOUD vs LOCAL)."""
    db = get_db()
    return {"mode": db.mode}

# Include Auth Router
from routes import auth_routes as auth_router
app.include_router(auth_router.router, prefix="/v1/auth", tags=["Auth"])

# --- Model Selection Endpoint ---

# --- Model Selection Endpoint ---


@app.get("/v1/models")
async def get_available_models():
    """Returns available io.net AI models for analysis."""
    default_model = os.getenv("IO_MODEL_NAME", "deepseek-ai/DeepSeek-V3.2")
    return {
        "default_model": default_model,
        "models": IO_NET_MODELS
    }

# --- Endpoints ---

@app.post("/v1/analyze")
async def analyze_code(request: AnalyzeRequest, current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Analyzes code and recommends GPU nodes.
    Protected by Hybrid Auth.
    """
    import time
    import os
    
    # Get model - use request model or fall back to env default
    default_model = os.getenv("IO_MODEL_NAME", "deepseek-ai/DeepSeek-V3.2")
    model_name = request.model if request.model else default_model
    api_base = os.getenv("IO_BASE_URL", "api.intelligence.io.solutions")
    
    pipeline_trace = []
    
    # === STEP 1: Auditor (LLM Analysis) ===
    step1_start = time.time()
    pipeline_trace.append({
        "step": 1,
        "agent": "Auditor",
        "status": "running",
        "action": "Sending code to LLM for analysis...",
        "details": {
            "model": model_name,
            "api": api_base.replace("https://", "").replace("/api/v1/", ""),
            "code_length": len(request.code)
        }
    })
    
    audit_report = await Auditor.analyze_code(request.code, model=model_name)
    step1_time = round(time.time() - step1_start, 2)
    
    pipeline_trace[0]["status"] = "completed"
    pipeline_trace[0]["duration_sec"] = step1_time
    pipeline_trace[0]["result"] = {
        "framework": audit_report.framework,
        "vram": f"{audit_report.vram_min_gb} GB",
        "health_score": audit_report.health_score,
        "issues_found": len(audit_report.critical_issues)
    }
    
    
    # Check Credits
    user_credits = float(current_user.get("credits", 0.0))
    if user_credits < 1.0: # Minimum 1 credit to analyze
        raise HTTPException(status_code=402, detail="Insufficient credits. Please top up.")

    # === STEP 2: Architect (Environment Planning) ===
    step2_start = time.time()
    pipeline_trace.append({
        "step": 2,
        "agent": "Architect",
        "status": "running",
        "action": "Analyzing imports and planning environment...",
        "details": {
            "framework": audit_report.framework,
            "vram_required": audit_report.vram_min_gb
        }
    })
    
    env_config = await Architect.plan_environment(
        framework=audit_report.framework,
        code=request.code,
        vram_gb=audit_report.vram_min_gb
    )
    step2_time = round(time.time() - step2_start, 2)
    
    pipeline_trace[1]["status"] = "completed"
    pipeline_trace[1]["duration_sec"] = step2_time
    pipeline_trace[1]["result"] = {
        "base_image": env_config.base_image,
        "packages_detected": len(env_config.python_packages),
        "cuda_version": env_config.cuda_version
    }
    
    # === STEP 3: Sniper (Market Analysis) ===
    step3_start = time.time()
    gpu_model = "RTX 4090" if audit_report.vram_min_gb > 20 else "RTX 3090"
    
    pipeline_trace.append({
        "step": 3,
        "agent": "Sniper",
        "status": "running",
        "action": "Fetching live GPU prices from io.net...",
        "details": {
            "api": "api.io.solutions/v1/io-explorer/network/market-snapshot",
            "target_gpu": gpu_model,
            "budget": f"${request.budget}/hr"
        }
    })
    
    best_nodes = await Sniper.get_best_nodes(
        budget_hourly=request.budget, 
        gpu_model=gpu_model
    )
    step3_time = round(time.time() - step3_start, 2)
    
    pipeline_trace[2]["status"] = "completed"
    pipeline_trace[2]["duration_sec"] = step3_time
    pipeline_trace[2]["result"] = {
        "nodes_found": len(best_nodes),
        "best_price": f"${best_nodes[0].price_hourly}/hr" if best_nodes else "N/A"
    }
    
    total_time = round(step1_time + step2_time + step3_time, 2)
    
    analysis_result = {
        "audit": audit_report.dict(),
        "environment": env_config.dict(),
        "market_recommendations": [node.dict() for node in best_nodes],
        "summary": {
            "framework": audit_report.framework,
            "vram_required": f"{audit_report.vram_min_gb} GB",
            "recommended_gpu": gpu_model,
            "estimated_setup": f"{env_config.estimated_setup_time_min} min",
            "health_score": audit_report.health_score
        },
        "pipeline_trace": {
            "total_duration_sec": total_time,
            "steps": pipeline_trace
        }
    }
    
    # Store in state for Chat Agent context
    state.last_analysis = analysis_result
    
    return analysis_result

# ...
try:
    from db.client import get_db
except ImportError:
    get_db = lambda: None

# ... (inside chat_agent)

@app.post("/v1/chat")
async def chat_agent(request: ChatRequest, current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Project-Aware AI Chatbot.
    Uses ChatAgent to provide context-aware responses (Analysis, Deployment, etc.)
    """
    from agents.chat import ChatAgent
    
    if not request.messages:
        raise HTTPException(status_code=400, detail="No messages provided")
    
    # 1. Persist User Message (Real DB)
    supabase = get_db()
    if supabase:
        try:
             supabase.table("io_chat_history").insert({
                "role": "user",
                "content": request.messages[-1]["content"]
            }).execute()
        except: pass

    # 2. Get Response from ChatAgent (Context Aware)
    # We can pass model preference if needed, for now use default
    response_content = await ChatAgent.chat(request.messages)
    
    # 3. Persist AI Response
    if supabase:
        try:
             supabase.table("io_chat_history").insert({
                "role": "assistant",
                "content": response_content
            }).execute()
        except: pass

    return {"role": "assistant", "content": response_content}

@app.get("/v1/market/status")
async def market_status():
    """
    [v1.0] Returns cached market data or status.
    """
    data = Sniper._get_market_data()
    return {"source": "live/cache", "node_count": len(data), "sample": data[:10]}

# --- Terminal WebSocket ---
from agents.terminal import terminal_manager
from fastapi import WebSocket, WebSocketDisconnect
import asyncio
import json

@app.websocket("/ws/terminal")
async def terminal_websocket(websocket: WebSocket):
    await websocket.accept()
    session_id = id(websocket) 
    
    try:
        # Wait for init message
        data = await websocket.receive_text()
        msg = json.loads(data)
        
        if msg.get("type") == "connect":
            config = msg.get("config")
            try:
                terminal_manager.create_session(session_id, config)
                await websocket.send_json({"type": "status", "data": "connected"})
            except Exception as e:
                await websocket.send_json({"type": "error", "data": str(e)})
                return
        
        # Main Loop
        session = terminal_manager.get_session(session_id)
        if not session:
            return

        # Background task to read from SSH
        async def read_ssh():
            while True:
                output = session.recv()
                if output:
                    await websocket.send_json({"type": "output", "data": output})
                else:
                    await asyncio.sleep(0.1)

        reader_task = asyncio.create_task(read_ssh())
        
        while True:
            data = await websocket.receive_text()
            msg = json.loads(data)
            
            if msg.get("type") == "input":
                session.send(msg.get("data"))
            elif msg.get("type") == "resize":
                # Handle resize if needed
                pass

    except WebSocketDisconnect:
        terminal_manager.close_session(session_id)
    except Exception as e:
        logger.error(f"WS Error: {e}")
        terminal_manager.close_session(session_id)

@app.post("/v1/deploy/simulate")
async def deploy_simulate(request: DeploySimulateRequest):
    """
    [v1.0] Starts a simulation job.
    """
    import uuid
    job_id = f"sim_{str(uuid.uuid4())[:8]}"
    return {"job_id": job_id, "status": "simulation_started"}

@app.post("/v1/deploy/live")
async def deploy_live(payload: dict):
    """
    [v1.0] Mock for Live Deployment.
    """
    return {"job_id": "live_999", "status": "pending_ssh_connection"}

class SSHConnectionRequest(BaseModel):
    hostname: str
    username: str
    port: int = 22
    auth_type: str = "key"  # "key" or "password"
    private_key: Optional[str] = None
    password: Optional[str] = None
    passphrase: Optional[str] = None  # For encrypted keys

@app.post("/v1/connection/test")
async def test_ssh_connection(request: SSHConnectionRequest):
    """
    [v1.0] Tests SSH connection to a remote node.
    Supports: Private Key, Password, and Passphrase-protected keys.
    """
    try:
        from services.ssh_manager import SSHManager
        success, message = await SSHManager.test_connection(
            hostname=request.hostname, 
            username=request.username, 
            port=request.port,
            auth_type=request.auth_type,
            private_key=request.private_key,
            password=request.password,
            passphrase=request.passphrase
        )
        return {"success": success, "message": message}
    except ImportError:
        return {"success": False, "message": "SSH Module (Paramiko) missing. Please rebuild backend."}
    except Exception as e:
        logger.error(f"SSH Test Error: {e}")
        return {"success": False, "message": f"Server Error: {str(e)}"}

# --- File Upload ---

ALLOWED_EXTENSIONS = {'.py', '.txt', '.yaml', '.yml', '.json', '.zip'}

def get_file_type(filename: str) -> str:
    """Determine file type from extension."""
    if filename.endswith('.py'):
        return 'script'
    elif filename.endswith('.txt'):
        return 'requirements'
    elif filename.endswith(('.yaml', '.yml')):
        return 'config'
    elif filename.endswith('.json'):
        return 'config'
    elif filename.endswith('.zip'):
        return 'archive'
    return 'other'

def find_entry_point(directory: str) -> Optional[str]:
    """Find the main Python script in a directory."""
    # Priority: main.py > train.py > run.py > first .py file
    priority_names = ['main.py', 'train.py', 'run.py', 'app.py', 'script.py']
    
    for name in priority_names:
        path = os.path.join(directory, name)
        if os.path.exists(path):
            return name
    
    # Fallback to first .py file
    for f in os.listdir(directory):
        if f.endswith('.py'):
            return f
    
    return None

@app.post("/v1/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    [v1.0] Uploads a single file for remote execution.
    Accepts: .py, .txt, .yaml, .yml, .json, .zip
    """
    # Security: Check allowed extensions
    ext = os.path.splitext(file.filename)[1].lower()
    if ext not in ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=400, 
            detail=f"Only {', '.join(ALLOWED_EXTENSIONS)} files are allowed"
        )
    
    # Generate unique filename
    file_id = str(uuid.uuid4())[:8]
    safe_filename = f"{file_id}_{file.filename.replace(' ', '_')}"
    file_path = os.path.join(UPLOADS_DIR, safe_filename)
    
    try:
        # Save file
        contents = await file.read()
        with open(file_path, "wb") as f:
            f.write(contents)
        
        file_url = f"http://localhost:8000/uploads/{safe_filename}"
        
        logger.info(f"File uploaded: {safe_filename} ({len(contents)} bytes)")
        
        return {
            "success": True,
            "filename": safe_filename,
            "original_name": file.filename,
            "size": len(contents),
            "url": file_url,
            "local_path": file_path,
            "type": get_file_type(file.filename)
        }
    except Exception as e:
        logger.error(f"Upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.post("/v1/upload/project")
async def upload_project(files: List[UploadFile] = File(...)):
    """
    [v1.0] Uploads multiple files or a ZIP archive as a project.
    Creates a project directory and extracts files.
    Returns project metadata including entry point.
    """
    import zipfile
    import io as io_module
    
    project_id = str(uuid.uuid4())[:8]
    project_dir = os.path.join(UPLOADS_DIR, f"project_{project_id}")
    os.makedirs(project_dir, exist_ok=True)
    
    uploaded_files = []
    
    try:
        for file in files:
            ext = os.path.splitext(file.filename)[1].lower()
            
            if ext == '.zip':
                # Extract ZIP archive
                contents = await file.read()
                try:
                    with zipfile.ZipFile(io_module.BytesIO(contents), 'r') as zip_ref:
                        # Security: Check for path traversal
                        for member in zip_ref.namelist():
                            if member.startswith('..') or member.startswith('/'):
                                raise HTTPException(status_code=400, detail="Invalid ZIP: path traversal detected")
                        
                        zip_ref.extractall(project_dir)
                        
                        # List extracted files
                        for root, dirs, files_list in os.walk(project_dir):
                            for f in files_list:
                                rel_path = os.path.relpath(os.path.join(root, f), project_dir)
                                full_path = os.path.join(root, f)
                                uploaded_files.append({
                                    "filename": rel_path,
                                    "size": os.path.getsize(full_path),
                                    "type": get_file_type(f),
                                    "source": "zip"
                                })
                        
                        logger.info(f"ZIP extracted: {file.filename} -> {len(uploaded_files)} files")
                except zipfile.BadZipFile:
                    raise HTTPException(status_code=400, detail="Invalid ZIP file")
            
            elif ext in ALLOWED_EXTENSIONS:
                # Regular file
                safe_filename = file.filename.replace(' ', '_')
                file_path = os.path.join(project_dir, safe_filename)
                
                contents = await file.read()
                with open(file_path, "wb") as f:
                    f.write(contents)
                
                uploaded_files.append({
                    "filename": safe_filename,
                    "size": len(contents),
                    "type": get_file_type(file.filename),
                    "source": "upload"
                })
                
                logger.info(f"File added to project: {safe_filename}")
            else:
                logger.warning(f"Skipped unsupported file: {file.filename}")
        
        # Find entry point
        entry_point = find_entry_point(project_dir)
        
        # Count files by type
        script_count = sum(1 for f in uploaded_files if f['type'] == 'script')
        
        return {
            "success": True,
            "project_id": project_id,
            "project_dir": project_dir,
            "files": uploaded_files,
            "file_count": len(uploaded_files),
            "script_count": script_count,
            "entry_point": entry_point,
            "has_requirements": any(f['type'] == 'requirements' for f in uploaded_files)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Project upload error: {e}")
        # Cleanup on error
        import shutil
        if os.path.exists(project_dir):
            shutil.rmtree(project_dir, ignore_errors=True)
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.get("/v1/uploads")
async def list_uploads():
    """
    [v1.0] Lists all uploaded files and projects.
    """
    try:
        files = []
        projects = []
        
        for item in os.listdir(UPLOADS_DIR):
            item_path = os.path.join(UPLOADS_DIR, item)
            
            if os.path.isdir(item_path) and item.startswith('project_'):
                # It's a project directory
                project_files = []
                for root, dirs, files_list in os.walk(item_path):
                    for f in files_list:
                        rel_path = os.path.relpath(os.path.join(root, f), item_path)
                        project_files.append(rel_path)
                
                projects.append({
                    "project_id": item.replace('project_', ''),
                    "path": item_path,
                    "file_count": len(project_files),
                    "entry_point": find_entry_point(item_path)
                })
            else:
                # Regular file
                ext = os.path.splitext(item)[1].lower()
                if ext in ALLOWED_EXTENSIONS:
                    files.append({
                        "filename": item,
                        "size": os.path.getsize(item_path),
                        "url": f"http://localhost:8000/uploads/{item}",
                        "type": get_file_type(item)
                    })
        
        return {"files": files, "projects": projects}
    except Exception as e:
        return {"files": [], "projects": [], "error": str(e)}


# --- Remote Execution ---

class ExecuteRequest(BaseModel):
    hostname: str
    username: str
    port: int = 22
    auth_type: str = "key"  # "key" or "password"
    private_key: Optional[str] = None
    password: Optional[str] = None
    passphrase: Optional[str] = None
    script_path: str  # Local path to the uploaded script


@app.post("/v1/deploy/execute")
async def deploy_execute(request: ExecuteRequest, current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Uploads script to remote server and executes it.
    Supports: Private Key, Password, and Passphrase-protected keys.
    """
    if not os.path.exists(request.script_path):
        raise HTTPException(status_code=404, detail="Script file not found")
    
    job_id = f"exec_{str(uuid.uuid4())[:8]}"
    
    if not hasattr(app, 'job_configs'):
        app.job_configs = {}
    
    app.job_configs[job_id] = {
        "hostname": request.hostname,
        "username": request.username,
        "port": request.port,
        "auth_type": request.auth_type,
        "private_key": request.private_key,
        "password": request.password,
        "passphrase": request.passphrase,
        "script_path": request.script_path,
        "status": "pending"
    }
    
    logger.info(f"Execution job created: {job_id}")
    
    return {
        "job_id": job_id,
        "status": "pending",
        "message": "Connect to WebSocket /ws/execute/{job_id} for logs"
    }


@app.websocket("/ws/execute/{job_id}")
async def websocket_execute(websocket: WebSocket, job_id: str):
    """
    [v1.0] Real-time execution logs via WebSocket.
    Uploads file to remote server and executes it.
    """
    await websocket.accept()
    
    try:
        if not hasattr(app, 'job_configs') or job_id not in app.job_configs:
            await websocket.send_text("‚ùå Job not found")
            await websocket.close()
            return
        
        config = app.job_configs[job_id]
        config["status"] = "running"
        
        await websocket.send_text(f"üöÄ Starting job: {job_id}")
        
        from services.ssh_manager import SSHManager
        
        async for line in SSHManager.upload_and_execute(
            hostname=config["hostname"],
            username=config["username"],
            local_path=config["script_path"],
            port=config["port"],
            auth_type=config["auth_type"],
            private_key=config.get("private_key"),
            password=config.get("password"),
            passphrase=config.get("passphrase")
        ):
            await websocket.send_text(line)
        
        config["status"] = "completed"
        await websocket.send_text(f"‚úÖ Job {job_id} completed")
        
    except Exception as e:
        logger.error(f"Execution error: {e}")
        await websocket.send_text(f"‚ùå Error: {str(e)}")
    finally:
        try:
            await websocket.close()
        except:
            pass


# --- Project Execution ---

class ExecuteProjectRequest(BaseModel):
    hostname: str
    username: str
    port: int = 22
    auth_type: str = "key"
    private_key: Optional[str] = None
    password: Optional[str] = None
    passphrase: Optional[str] = None
    project_dir: str  # Local path to the project directory
    entry_point: str  # Main script to execute (e.g., "main.py")
    install_requirements: bool = True


@app.post("/v1/deploy/project")
async def deploy_project(request: ExecuteProjectRequest):
    """
    [v1.0] Uploads a project to remote server and executes it.
    Supports: Multiple files, requirements.txt installation, entry point execution.
    """
    if not os.path.exists(request.project_dir):
        raise HTTPException(status_code=404, detail="Project directory not found")
    
    entry_path = os.path.join(request.project_dir, request.entry_point)
    if not os.path.exists(entry_path):
        raise HTTPException(status_code=404, detail=f"Entry point '{request.entry_point}' not found in project")
    
    job_id = f"proj_{str(uuid.uuid4())[:8]}"
    
    if not hasattr(app, 'job_configs'):
        app.job_configs = {}
    
    app.job_configs[job_id] = {
        "type": "project",
        "hostname": request.hostname,
        "username": request.username,
        "port": request.port,
        "auth_type": request.auth_type,
        "private_key": request.private_key,
        "password": request.password,
        "passphrase": request.passphrase,
        "project_dir": request.project_dir,
        "entry_point": request.entry_point,
        "install_requirements": request.install_requirements,
        "status": "pending"
    }
    
    logger.info(f"Project execution job created: {job_id}")
    
    return {
        "job_id": job_id,
        "status": "pending",
        "message": "Connect to WebSocket /ws/execute-project/{job_id} for logs"
    }


@app.websocket("/ws/execute-project/{job_id}")
async def websocket_execute_project(websocket: WebSocket, job_id: str):
    """
    [v1.0] Real-time project execution logs via WebSocket.
    Uploads entire project, installs requirements, and executes entry point.
    """
    await websocket.accept()
    
    try:
        if not hasattr(app, 'job_configs') or job_id not in app.job_configs:
            await websocket.send_text("‚ùå Job not found")
            await websocket.close()
            return
        
        config = app.job_configs[job_id]
        
        if config.get("type") != "project":
            await websocket.send_text("‚ùå Invalid job type for this endpoint")
            await websocket.close()
            return
        
        config["status"] = "running"
        
        await websocket.send_text(f"üöÄ Starting project job: {job_id}")
        
        from services.ssh_manager import SSHManager
        
        async for line in SSHManager.upload_project_and_execute(
            hostname=config["hostname"],
            username=config["username"],
            project_dir=config["project_dir"],
            entry_point=config["entry_point"],
            port=config["port"],
            auth_type=config["auth_type"],
            private_key=config.get("private_key"),
            password=config.get("password"),
            passphrase=config.get("passphrase"),
            install_requirements=config.get("install_requirements", True)
        ):
            await websocket.send_text(line)
        
        config["status"] = "completed"
        await websocket.send_text(f"‚úÖ Project job {job_id} completed")
        
    except Exception as e:
        logger.error(f"Project execution error: {e}")
        await websocket.send_text(f"‚ùå Error: {str(e)}")
    finally:
        try:
            await websocket.close()
        except:
            pass

@app.websocket("/ws/logs/{job_id}")
async def websocket_logs(websocket: WebSocket, job_id: str):
    """
    [v1.0] Real-time logs for jobs (Simulation/Live).
    """
    await websocket.accept()
    try:
        async for line in Executor.run_simulation({"job_id": job_id}):
             await websocket.send_text(line)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        try:
            await websocket.close()
        except:
            pass

@app.get("/v1/dashboard/stats")
async def dashboard_stats(current_user: dict = Depends(get_current_user)):
    """
    [v1.0] Aggregated real-time stats for the dashboard.
    Returns: Agents, Market Status, Financials, System Health.
    """
    # 1. Market Data (Real from io.net)
    market_data = Sniper._get_market_data()
    node_count = sum(n.get("total_nodes", 0) for n in market_data) if market_data else 0
    if node_count == 0: node_count = len(market_data) # Fallback if total_nodes missing

    # 2. Financials (Real from DB)
    real_balance = float(current_user.get("credits", 0.0))
    # We remove the mock burn rate calculation as requested
    est_hourly_burn = 0.0 
    # Estimate burn: sum of active job budgets or placeholder
    est_hourly_burn = 0.45 * 4 # Example: 4 active nodes x $0.45 avg

    # 3. Agent Status
    # In a real system, we'd check if specific asyncio tasks are running.
    # Here we assume core agents are "Active" if the app is up.
    active_agents = 4 # Auditor, Sniper, Architect, Executor

    # 4. Alerts
    # Count recent error logs
    recent_logs = state.get_agent_logs(limit=20)
    error_count = sum(1 for log in recent_logs if "error" in str(log).lower() or "warning" in str(log).lower())

    return {
        "active_agents": active_agents,
        "market_nodes": node_count,
        "market_sample": market_data[:20], # Top 20 for snapshot
        "system_health": 100 if error_count == 0 else 90, # Simple logic
        "alerts_count": error_count,
        "financials": {
            "balance": real_balance,
            "currency": "USDC",
            "hourly_burn": est_hourly_burn,
            "rewards_24h": 0.0 # Placeholder/Future implementation
        }
    }


# ==========================================
# üèÅ End of v1.0 Core
# ==========================================

